{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae59ebcb",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "### Кратко о гипеплоскости\n",
    "\n",
    "Уравнение гиперплоскости:\n",
    "$$\n",
    "    \\pi(w, b) = <x, w> + b\n",
    "$$\n",
    "\n",
    "Расстояние между вектором $x$ и гипеплоскостью $\\pi(w, b)$ рассчитывается следующим образом:\n",
    "$$\n",
    "    \\rho(x, \\pi) = \\dfrac{<w, x> + b}{||w||}, \\rho >(<)0, \\text{если } x \\in (R^n)_{\\pi}^{+(-)}, \\rho = 0, \\text{если } x \\in \\pi \n",
    "$$\n",
    "\n",
    "Некоторые наблюдения по поводу гиперплоскости:\n",
    "   * начало кординат находится в положительной полуплоскости, если $b > 0$, в отрицательной, если $b < 0$, иначе если $b = 0$, то гиперплоскость пересекает начало координат;\n",
    "   * увеличивая $|b|$ мы двигаем гиперплоскость параллельно себе вдоль вектора нормали;\n",
    "   * изменяя вектор w, сохраняя $|w| = 1$, мы поворачиваем гиперплоскость вокруг начала координат, причем радиус данной окружности равен $\\dfrac{|b|}{||w||}$;\n",
    "   * изменяя длину вектора $w$, не меняя его направления, мы двигаем гиперплоскость параллельно себе относительно начала координат. \n",
    "   \n",
    "### Разделяющая гиперплоскость\n",
    "\n",
    "Гиперплоскость $\\pi(w, b)$ называется _разделяющей_, если для любого класса $C_1, \\ \\ C_2$ выполняется:\n",
    "   * $<w, x> + b > 0, \\ \\ x \\in C_1$;\n",
    "   * $<w, x> + b < 0, \\ \\ x \\in C_2$.\n",
    "   \n",
    "или\n",
    "\n",
    "   * $<w, x> + b < 0, \\ \\ x \\in C_1$;\n",
    "   * $<w, x> + b > 0, \\ \\ x \\in C_2$.\n",
    "   \n",
    "Два класса называются _линейно разделимыми_, если существует как минимум одна гиперплоскость, разделяющая их.\n",
    "\n",
    "Если классы линейно разделимы, то решающая функция будет иметь вид:\n",
    "$$\n",
    "    f(x) = sign\\{<w, x> + b\\}\n",
    "$$\n",
    "\n",
    "Известно, что для линейно разделимых классов существует бесконечное число гиперплоскостей, разделяющих их (можно просто менять $b$ или $w$). Так вот $SVM$ среди них выбирает ту, которая имеет максимальный __отступ__($m$, _margin_) от каждого от классов:\n",
    "$$\n",
    "    m(\\pi, C_1, C_2) = \\rho(\\pi, C_1) + \\rho(\\pi, C_2), \\quad \\rho(\\pi, C) = \\min\\limits_{x \\in C}|\\rho(\\pi, x)|\n",
    "$$\n",
    "Так же, помимо такой формулы, отступ можно понимать как минимальное расстояние между двумя классами:\n",
    "$$\n",
    "    m(\\pi, C_1, C_2) = \\rho(C_{1}^{w}, C_{2}^{w}) = \\min\\limits_{x_1 \\in C_{1}^{w}, x_2 \\in C_{2}^{w}}\n",
    "$$\n",
    "\n",
    "Нужно знать два свойства отступа:\n",
    "   * отступ зависит только от вектора $w$, но не от $b$;\n",
    "   * для каждой гиперплоскости, разделяющей два класса:\n",
    "   $$\n",
    "       m(\\pi, C_1, C_2) \\leq \\rho(C_1, C_2)\n",
    "   $$\n",
    "   \n",
    "### Максимизация отступа\n",
    "\n",
    "#### Прямая оптимизационная задача\n",
    "$$\n",
    "    \\dfrac{1}{2}||w||^2 \\rightarrow \\min\\limits_{w, b} \\newline\n",
    "    <w, x> + b \\geq 1, \\forall x \\in C_1, \\newline\n",
    "    <w, x> + b \\leq -1, \\forall x \\in C_2\n",
    "$$\n",
    "\n",
    "Ввиду ___строгой выпуклости___ целевой функции (строго выпукла ввиду того, что Гессиан, т. е. матрица вторых производных, положительно определена) и _выпуклости_ множества ограничений, наша задача будет иметь единственное оптимальное решение $w^*, b^*$. \n",
    "\n",
    "Теперь разберемся, почему параметры $w, b$ могут быть найдены из оптимизационной задачи выше. Немного изменим исходную задачу. \n",
    "$$\n",
    "    \\dfrac{1}{||w||} \\rightarrow \\max\\limits_{w, b} \\newline\n",
    "    \\dfrac{<w, x> + b}{||w||} \\geq \\dfrac{1}{||w||}, \\forall x \\in C_1, \\newline\n",
    "    \\dfrac{<w, x> + b}{||w||} \\leq -\\dfrac{1}{||w||}, \\forall x \\in C_2, \\newline\n",
    "$$\n",
    "\n",
    "А это то же самое, что и:\n",
    "$$\n",
    "    d \\rightarrow \\max\\limits_{w, b} \\newline\n",
    "    \\rho(\\pi, x) \\geq d, \\forall x \\in C_1, \\newline\n",
    "    \\rho(\\pi, x) \\leq -d, \\forall x \\in C_2, \\newline\n",
    "    d = \\dfrac{1}{||w||}\n",
    "$$\n",
    "\n",
    "<details>\n",
    "    <summary style=\"display: list-item\">Геометрическое объяснение </summary>\n",
    "    <p>Geometrically, connection between parameters w, b and d can be illustrated in the\n",
    "following way. Let us draw a spherical hull of radius d = 1/||w|| around each training\n",
    "point x. Consider some feasible hyperplane π(w, b). According to constraints (2.11),\n",
    "(2.12), this hyperplane must separate our points together with their hulls (Figure 6a).\n",
    "Now suppose we want to increase d twofold. Since d is a function of ||w||, we have to\n",
    "decrease ||w|| twofold. If we divide vector w by two, we move our hyperplane parallel to\n",
    "itself further from the origin. However, if we divide by two vector w and intercept b, we\n",
    "do not move the hyperplane. This way, downscaling w and b, we increase the radius of\n",
    "the hulls while keeping hyperplane π in the same position and orientation, until at least\n",
    "one hull touches it (Figure 6b). If we have space to move π parallel to itself away from the\n",
    "hull that touches it, we can do it by changing b only, and let the hulls grow further. At\n",
    "some point, our hulls will reach maximum size d achievable for hyperplanes with normal\n",
    "vectors collinear to w (Figure 6c). If we have space for the hulls to grow further, we can\n",
    "change orientation of π by changing components of vector w, while keeping ||w|| equal to\n",
    "the current value of 1/d (see the end of subsection 1.2). Doing so and adjusting b, we keep\n",
    "π feasible and increase d until we arrive to the optimal configuration (Figure 6d).</p>\n",
    "</details>\n",
    "\n",
    "### Опорные вектора\n",
    "\n",
    "Если $(w, b)$ -- решение прямой задачи, а $\\alpha$ -- двойственной, то они связаны следующим образом:\n",
    "$$\n",
    "    w = \\sum\\limits_{i = 1}^{l}\\alpha_{i} \\cdot y_{i} \\cdot x_{i}\n",
    "$$\n",
    "\n",
    "Это значит, что нормальный вектор $w$ является комбинацией векторов $x_i$ с коэфициентами $\\alpha_{i} \\cdot y_{i}$, для которых:\n",
    "$$\n",
    "    \\sum\\limits_{i = 1}^{l}\\alpha_{i} \\cdot y_{i} = 0, \\quad \\alpha_i \\geq 0, y_i \\in \\{-1, 1\\}.\n",
    "$$\n",
    "Вектора $x_i$, для которых $\\alpha_i > 0$, называются ___опорными___. Опорным называется вектор, ближайший до разделяющей гиперплоскости, и расстояние между ними равно $\\dfrac{1}{||w||}$. Опорные вектора лежат на отступе(margin) разделяющей гиперплоскости.\n",
    "\n",
    "Вектор лежит на margin-e, если:\n",
    "$$\n",
    "    <w, x> + b = 1 \\quad \\text{или } \\quad <w, x> + b = -1\n",
    "$$\n",
    "\n",
    "__Нужно отметить__, что среди всех объектов одного класса, объекты, находящиеся ближе всего к объектам другого класса, намного сложнее классифицировать, так как их можно спутать с объектами другого класса. Важное свойство SVM: среди всех объектов выборки наибольшее значение для SVM имеют те объекты, которые лежат на margin, в то время как другие объекты, находящиеся в глубине одного из классов, не имеют серьезного значения. ___Следствием___ из этого факта является то, что SVM нечувствителен к выбросам, находящимся вдали от margin.\n",
    "\n",
    "#### Сколько опорных векторов может иметь SVM?\n",
    "Минимум два. Это вытекает из данного факта:\n",
    "$$\n",
    "    \\sum\\limits_{i:\\ \\ y_{i} = 1}\\alpha_{i} = \\sum\\limits_{j:\\ \\ y_j = -1}\\alpha_{j}\n",
    "$$\n",
    "Максимум - $l$, то есть размер выборки.\n",
    "\n",
    "_Большое количество опорных векторов_ является возможным фактором переобучения.\n",
    "\n",
    "### Гиперплоскость с максимальным margin для линейно неразделимой выборки\n",
    "\n",
    "#### Прямая задача\n",
    "\n",
    "Прорелаксируем ограничения прямой задачи в случае линейно разделимой выборки:\n",
    "$$\n",
    "    \\dfrac{1}{2}\\sum\\limits_{k = 1}^{n}w_{k}^{2} + C\\sum\\limits_{i = 1}^{l}\\xi_{i} \\rightarrow \\min\\limits_{w, \\xi, b} \\newline\n",
    "    y_{i} \\cdot \\left(\\sum\\limits_{k = 1}^{n}x_{ik}w_{k}\\right) \\geq 1 - \\xi_{i}, \\newline\n",
    "    \\xi_{i} \\geq 0;\n",
    "$$\n",
    "\n",
    "В данном случае margin определяется аналогично случаю линейной разделимости: margin - это регион между\n",
    "$$\n",
    "    <w^*, x> + b^* = 1 \\quad\\ \\text {и } \\quad <w^*, x> + b= -1\n",
    "$$\n",
    "\n",
    "#### Немного о параметре $C$\n",
    "\n",
    "У параметра $C$ есть несколько очень полезных свойств:\n",
    "   * балансирует две важные цели: максимизация отступа и минимизация суммарной ошибки;\n",
    "   * значение параметра $C$ зависит от размеров выборки и разброса значений данных. Чтобы сделать его независимым от рамзера выборки, мы целевую функцию заменяем на $\\dfrac{1}{2}\\sum\\limits_{k = 1}^{n}w_{k}^{2} + C\\dfrac{1}{l}\\sum\\limits_{i = 1}^{l}\\xi_{i} \\rightarrow \\min\\limits_{w, \\xi, b}$\n",
    "   * лучше подбиратбь данный параметр по логарифмической шкале;\n",
    "   * также его можно использовать в тех случаях, когда цена ошибки на объектах разных классов разная. В таком случае исходную целевую функцию мы заменяем на следующую:\n",
    "   $$\n",
    "       \\dfrac{1}{2}\\sum\\limits_{k = 1}^{n}w_{k}^{2} + C_1\\sum\\limits_{y_i = 1,i = 1}^{l}\\xi_{i} + C_2\\sum\\limits_{y_i = -1,i = 1}^{l}\\xi_{i} \\rightarrow \\min\\limits_{w, \\xi, b}\n",
    "   $$\n",
    "   * также данный трюк используется в ситуации с дисбалансом классов. В таком случае мы просто увеличиваем цену ошибки на объектах малого класса.\n",
    "   \n",
    "#### Опорные вектора\n",
    "\n",
    "В случае линейно неразделимой выборки, в отличие от линейно разделимой, опорные вектора могут находиться не только на margin между двумя классами, но также и внутри него между двумя классами, и снаружи margin на неправильной стороне относительно гиперплоскости. \n",
    "\n",
    "И здесь количество опорных векторов зависит от параметра $C$: когда мы уменьшаем его, margin увеличивается и количество опорных векторов может возрастать.\n",
    "\n",
    "### SVM с ядрами\n",
    "\n",
    "В контексте SVM ядро $k(x, z)$ - это специальная функция, которая используется вместо стандартного скалярного произведения $<x, z>$. Оно позволяет для разделения классов использовать не только гиперплоскость, но также и дургое большое число разделяющих поверхностей.\n",
    "\n",
    "Когда мы используем ядро, мы определяем явно новое признаковое пространство $H$ и признаковое отображение $\\Phi: R^n \\rightarrow H$\n",
    "\n",
    "#### Смена ядра\n",
    "\n",
    "Исходную целевую функцию в двойственной задаче в неудобном для нас пространстве мы можем заменить на следующую:\n",
    "$$\n",
    "    \\sum\\limits_{i = 1}^{l}\\alpha_{i} - \\dfrac{1}{2}\\sum\\limits_{i = 1}^{l}\\sum\\limits_{j = 1}^{l}\\alpha_{i}\\alpha_{j}<\\Phi(x_i), \\Phi(x_j)> \\rightarrow \\max\\limits_{\\alpha}\n",
    "$$\n",
    "\n",
    "$k(x, z)$, используемое вместо выражения $<\\Phi(x), \\Phi(z)>$, называется ядерной функцией относительно отображения $\\Phi$.\n",
    "\n",
    "#### Ядра\n",
    "\n",
    "__Свойства__ ядра:\n",
    "   * является симметричной функцией, то есть $k(x, z) = k(z, x)$; \n",
    "   * функция $k(x, z)$, определяющая положительно определнную матрицу Грама $K(z_1, \\dots, z_m) = (k(z_i, z_j))$, являеься положительно определенной.\n",
    "\n",
    "Свойства выше иногда называются _свойствами Меркера_.\n",
    "\n",
    "Функция ядра не определяется единсвтенным образом пространством $H$ и отображением $\\Phi$.\n",
    "\n",
    "##### Полиномиальные ядра\n",
    "\n",
    "$$\n",
    "    k(x, z) = <x, z>^{d}, \\quad d \\geq 2\n",
    "$$\n",
    "$$\n",
    "    <x, z>^{d} = \\left(\\sum\\limits_{i = 1}^{n}x_{i}z_{i}\\right)^{d}\n",
    "$$\n",
    "Теперь ключевая часть целевой функции будет выглядеть следующим образом:\n",
    "$$\n",
    "    \\sum\\limits_{i = 1}^{l}\\alpha_{i}y_{i}(q<x, x_{i}> + p)^{d}\n",
    "$$\n",
    "\n",
    "#####  Gaussian radial basis function kernel\n",
    "\n",
    "$$\n",
    "    k(x, z) = \\exp^{-\\frac{||x - z||^{2}}{2\\sigma^{2}}} + b\n",
    "$$\n",
    "\n",
    "Любое признаковое пространство, соответствующее гауссовскому преобразованию, является бесконечномерным. \n",
    "\n",
    "Теперь ключевая часть целевой функции будет выглядеть следующим образом:\n",
    "$$\n",
    "    \\sum\\limits_{i = 1}^{l}\\alpha_{i}y_{i}\\exp^{-\\frac{||x - x_{i}||^{2}}{2\\sigma^{2}}} + b\n",
    "$$\n",
    "\n",
    "Значение $\\sigma$ должно варьироваться от минимального до максимального расстояния объектов выборки друг от друга."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
